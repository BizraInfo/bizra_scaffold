"""
Autonomous Giants + Graph-of-Thoughts pipeline runner.

Builds a user profile from chat history, extracts Giants Protocol wisdom,
runs GoT reasoning, and writes evidence artifacts for verification.
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import sys
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

try:
    import yaml
except ImportError:  # pragma: no cover - optional dependency in runtime
    yaml = None

REPO_ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(REPO_ROOT))

from core.architecture.modular_components import ConvergenceQuality, ConvergenceResult
from core.knowledge.giants_enhanced_got import GiantsEnhancedGoT
from core.knowledge.giants_protocol import WisdomSource, create_giants_engine
from core.memory.agent_memory import AgentMemorySystem
from core.snr_scorer import SNRScorer

from scripts import ingest_user_profile

# Default path for memory persistence
DEFAULT_MEMORY_SNAPSHOT = "data/checkpoints/memory_snapshot.json"

logger = logging.getLogger("bizra.giants_got_pipeline")


DEFAULT_CHAT_DIR = r"C:\bizra_scaffold\chat data sample"
DEFAULT_PROFILE_PATH = "config/user_profile_derived.yaml"
DEFAULT_QUERY = (
    "Unify power, knowledge, and progress with high-SNR interdisciplinary reasoning."
)
STOP_KEYWORDS = {
    "self",
    "system",
    "data",
    "from",
    "with",
    "all",
    "none",
    "true",
    "false",
    "file",
    "class",
    "const",
    "def",
    "return",
    "import",
    "function",
    "div",
    "https",
}
INTERDISCIPLINARY_TERMS = [
    "ethics",
    "cryptography",
    "economics",
    "governance",
    "systems",
    "design",
    "validation",
    "policy",
    "evidence",
    "observability",
    "performance",
    "security",
    "architecture",
]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run an autonomous Giants->GoT pipeline and emit evidence artifacts."
    )
    parser.add_argument("--chat-dir", default=DEFAULT_CHAT_DIR)
    parser.add_argument("--profile-path", default=DEFAULT_PROFILE_PATH)
    parser.add_argument("--skip-ingest", action="store_true")
    parser.add_argument("--output-dir", default=None)
    parser.add_argument("--query", default=DEFAULT_QUERY)
    parser.add_argument("--top-k-chains", type=int, default=5)
    parser.add_argument("--beam-width", type=int, default=8)
    parser.add_argument("--max-depth", type=int, default=4)
    parser.add_argument("--resource-budget", type=float, default=1.0)
    parser.add_argument("--seed-topics", type=int, default=6)
    parser.add_argument("--hub-topics", type=int, default=4)
    parser.add_argument("--log-level", default="INFO")
    parser.add_argument(
        "--memory-snapshot",
        default=DEFAULT_MEMORY_SNAPSHOT,
        help="Path to memory snapshot JSON for persistence across runs",
    )
    parser.add_argument(
        "--no-memory-persist",
        action="store_true",
        help="Disable memory persistence (don't load/save memory snapshot)",
    )
    return parser.parse_args()


def load_profile(profile_path: Path) -> Dict[str, Any]:
    if not profile_path.exists():
        return {}
    if yaml is None:
        raise RuntimeError("PyYAML is required to load profile data.")
    with profile_path.open("r", encoding="utf-8") as handle:
        return yaml.safe_load(handle) or {}


def write_profile(profile_data: Dict[str, Any], profile_path: Path) -> None:
    if yaml is None:
        logger.warning("PyYAML not available, skipping profile write.")
        return
    profile_path.parent.mkdir(parents=True, exist_ok=True)
    with profile_path.open("w", encoding="utf-8") as handle:
        handle.write("# Derived User Profile from Chat History\n")
        handle.write("# Generated by scripts/run_giants_got_pipeline.py\n")
        yaml.dump(profile_data, handle, allow_unicode=False, sort_keys=True)


def load_memory_snapshot(snapshot_path: Path, snr_scorer: SNRScorer) -> Optional[AgentMemorySystem]:
    """Load memory system from a persisted snapshot if it exists."""
    if not snapshot_path.exists():
        logger.info("No memory snapshot found at %s, starting fresh.", snapshot_path)
        return None

    try:
        memory_system = AgentMemorySystem(snr_scorer=snr_scorer)
        memory_system.import_from_json(str(snapshot_path))
        stats = memory_system.get_statistics()
        logger.info(
            "Loaded memory snapshot: %d total items across tiers",
            stats.get("total_memories", 0),
        )
        return memory_system
    except Exception as e:
        logger.warning("Failed to load memory snapshot: %s, starting fresh.", e)
        return None


def save_memory_snapshot(memory_system: AgentMemorySystem, snapshot_path: Path) -> bool:
    """Persist memory system to disk."""
    try:
        snapshot_path.parent.mkdir(parents=True, exist_ok=True)
        memory_system.export_to_json(str(snapshot_path))
        stats = memory_system.get_statistics()
        logger.info(
            "Saved memory snapshot: %d items to %s",
            stats.get("total_memories", 0),
            snapshot_path,
        )
        return True
    except Exception as e:
        logger.error("Failed to save memory snapshot: %s", e)
        return False


async def inject_wisdom_to_memory(
    memory_system: AgentMemorySystem,
    enhanced_chains: List[Any],
    giants_engine: Any,
) -> int:
    """Store extracted wisdom and enhanced chains in memory for future runs."""
    from core.memory.agent_memory import MemoryTier
    
    injected_count = 0

    # Store each enhanced chain as episodic memory
    for chain in enhanced_chains:
        try:
            content = (
                f"Enhanced reasoning chain: {chain.chain.id}\n"
                f"SNR: {chain.enhanced_snr:.3f}, Giants boost: {chain.giants_boost:.3f}\n"
                f"Thoughts: {len(chain.chain.thoughts)}, Bridges: {len(chain.chain.bridges)}\n"
                f"Wisdom augmentations: {len(chain.wisdom_augmentations)}"
            )
            await memory_system.remember(
                content=content,
                tier=MemoryTier.EPISODIC,
                metadata={
                    "chain_id": chain.chain.id,
                    "enhanced_snr": chain.enhanced_snr,
                    "giants_boost": chain.giants_boost,
                    "domains": list(
                        {d for w in chain.wisdom_augmentations for d in w.domain_tags}
                    ),
                    "wisdom_count": len(chain.wisdom_augmentations),
                },
                snr_score=chain.enhanced_snr,
            )
            injected_count += 1
        except Exception as e:
            logger.debug("Failed to store chain in memory: %s", e)

    # Store hub concepts as semantic memory
    for concept, data in giants_engine.HUB_CONCEPTS.items():
        try:
            await memory_system.remember(
                content=f"Hub concept: {concept} - Domains: {', '.join(data['domains'])}",
                tier=MemoryTier.SEMANTIC,
                domains=set(data["domains"]),
                metadata={
                    "concept": concept,
                    "type": "hub_concept",
                },
            )
            injected_count += 1
        except Exception as e:
            logger.debug("Failed to store hub concept in memory: %s", e)

    return injected_count


def build_profile(chat_dir: Path, profile_path: Path, skip_ingest: bool) -> Dict[str, Any]:
    if skip_ingest and profile_path.exists():
        logger.info("Loading existing profile: %s", profile_path)
        return load_profile(profile_path)

    if not chat_dir.exists():
        raise FileNotFoundError(f"Chat directory not found: {chat_dir}")

    logger.info("Scanning chat directory: %s", chat_dir)
    files = ingest_user_profile.load_chat_files(str(chat_dir))
    all_messages: List[str] = []
    for file_path in files:
        all_messages.extend(ingest_user_profile.extract_user_data(file_path))

    if not all_messages:
        raise RuntimeError("No user messages extracted from chat data.")

    keywords, topics = ingest_user_profile.analyze_user_profile(all_messages)
    profile_data = {
        "total_messages_analyzed": len(all_messages),
        "top_topics": dict(topics.most_common()),
        "top_keywords": dict(keywords.most_common(50)),
    }

    write_profile(profile_data, profile_path)
    return profile_data


def summarize_profile(profile_data: Dict[str, Any], top_n: int = 8) -> Dict[str, Any]:
    topics = profile_data.get("top_topics", {})
    keywords = profile_data.get("top_keywords", {})
    return {
        "total_messages_analyzed": profile_data.get("total_messages_analyzed", 0),
        "top_topics": dict(list(topics.items())[:top_n]),
        "top_keywords": dict(list(keywords.items())[:top_n]),
    }


def build_reasoning_trace(profile_data: Dict[str, Any], query: str) -> Dict[str, Any]:
    topics = list(profile_data.get("top_topics", {}).keys())
    decisions = []
    explorations = []
    for idx, topic in enumerate(topics[:6]):
        decisions.append(
            {
                "type": "design",
                "content": f"Prioritize {topic} in system architecture.",
            }
        )
        explorations.append(
            {
                "type": "option",
                "content": f"Option {chr(65 + idx)}: combine {topic} with governance.",
            }
        )

    constraints = [
        {
            "name": "ihsan_threshold",
            "description": "IM >= 0.95",
            "strength": 0.95,
            "concepts": ["ihsan", "ethics"],
            "domains": ["ethics", "validation"],
        }
    ]
    invariants = [
        {
            "name": "fail_closed",
            "assertion": "Reject on uncertainty and low-ihsan outputs.",
            "concepts": ["safety", "security"],
        }
    ]
    domain_crossings = [
        {
            "from": "architecture",
            "to": "security",
            "insight": "Security as a first-class architecture layer.",
            "novelty": 0.8,
            "strength": 0.75,
            "concepts": ["architecture", "security"],
        },
        {
            "from": "optimization",
            "to": "governance",
            "insight": "Performance budgets guided by governance policies.",
            "novelty": 0.7,
            "strength": 0.7,
            "concepts": ["optimization", "governance"],
        },
    ]

    return {
        "query": query,
        "decisions": decisions,
        "explorations": explorations,
        "constraints": constraints,
        "invariants": invariants,
        "domain_crossings": domain_crossings,
    }


def _tokenize(concept: str) -> Set[str]:
    return {token for token in concept.replace("-", "_").split("_") if token}


def derive_domains(concept: str, hub_domains: Dict[str, Set[str]]) -> Set[str]:
    concept_key = concept.lower()
    if concept_key in hub_domains:
        return set(hub_domains[concept_key])

    custom = {
        "pat": {"agentic", "governance"},
        "pat_sat": {"agentic", "governance"},
        "sat": {"verification", "governance"},
        "agent": {"agentic"},
        "agentic": {"agentic"},
        "bizra": {"system", "mission"},
        "optimization": {"performance", "systems"},
        "python": {"software", "programming"},
        "rust": {"systems", "programming"},
        "architecture": {"design", "system"},
        "security": {"safety", "cryptography"},
        "interdisciplinary": {"synthesis", "research"},
        "governance": {"policy", "operations"},
        "memory": {"knowledge", "retention"},
        "progress": {"delivery", "operations"},
        "power": {"execution", "infrastructure"},
        "ethics": {"ethics", "validation"},
        "cryptography": {"cryptography", "security"},
        "economics": {"economics", "systems"},
        "validation": {"validation", "safety"},
        "policy": {"policy", "governance"},
        "evidence": {"evidence", "verification"},
        "observability": {"monitoring", "operations"},
        "performance": {"performance", "systems"},
        "design": {"design", "architecture"},
        "systems": {"systems", "architecture"},
    }

    if concept_key in custom:
        return set(custom[concept_key])

    tokens = _tokenize(concept_key)
    domains: Set[str] = set()
    for token in tokens:
        if token in custom:
            domains.update(custom[token])
    return domains or {"general"}


def build_domain_map(concepts: Iterable[str], hub_domains: Dict[str, Set[str]]) -> Dict[str, Set[str]]:
    return {concept: derive_domains(concept, hub_domains) for concept in concepts}


def build_adjacency(
    concepts: List[str],
    domains_by_concept: Dict[str, Set[str]],
    max_neighbors: int = 6,
) -> Dict[str, List[Dict[str, Any]]]:
    adjacency: Dict[str, List[Dict[str, Any]]] = {}
    token_cache = {concept: _tokenize(concept) for concept in concepts}

    for concept in concepts:
        scored_neighbors: List[Tuple[int, str, int]] = []
        bridge_candidates: List[str] = []
        for other in concepts:
            if other == concept:
                continue
            domain_overlap = len(domains_by_concept[concept] & domains_by_concept[other])
            token_overlap = len(token_cache[concept] & token_cache[other])
            score = domain_overlap * 2 + token_overlap
            if domain_overlap == 0:
                bridge_candidates.append(other)
            if score > 0:
                scored_neighbors.append((score, other, domain_overlap))

        if not scored_neighbors:
            fallback = [c for c in concepts if c != concept][:max_neighbors]
            scored_neighbors = [(0, other, 0) for other in fallback]

        scored_neighbors.sort(key=lambda x: (-x[0], x[1]))
        neighbors: List[Dict[str, Any]] = []
        neighbor_overlaps: Dict[str, int] = {}
        for _, neighbor, overlap in scored_neighbors[:max_neighbors]:
            relation = "BRIDGE" if overlap == 0 else "RELATED_TO"
            consistency = min(0.95, 0.6 + 0.08 * overlap)
            disagreement = max(0.05, 0.35 - 0.05 * overlap)
            neighbors.append(
                {
                    "id": neighbor,
                    "domains": sorted(domains_by_concept[neighbor]),
                    "relation_types": [relation],
                    "weight": min(1.0, 0.5 + 0.1 * overlap),
                    "consistency": consistency,
                    "disagreement": disagreement,
                    "ihsan": 0.96,
                }
            )
            neighbor_overlaps[neighbor] = overlap

        if bridge_candidates:
            bridge_candidates.sort()
            bridge_neighbor = bridge_candidates[0]
            if bridge_neighbor not in neighbor_overlaps:
                bridge_entry = {
                    "id": bridge_neighbor,
                    "domains": sorted(domains_by_concept[bridge_neighbor]),
                    "relation_types": ["BRIDGE"],
                    "weight": 0.45,
                    "consistency": 0.55,
                    "disagreement": 0.35,
                    "ihsan": 0.96,
                }
                if len(neighbors) < max_neighbors:
                    neighbors.append(bridge_entry)
                else:
                    neighbors[-1] = bridge_entry
        adjacency[concept] = neighbors

    return adjacency


def build_concept_pool(
    seed_concepts: List[str],
    profile_data: Dict[str, Any],
    term_signals: Dict[str, float],
    keyword_limit: int = 12,
) -> List[str]:
    keywords = list(profile_data.get("top_keywords", {}).keys())
    keyword_candidates = [
        k
        for k in keywords
        if k.isalpha() and len(k) >= 4 and k not in STOP_KEYWORDS
    ]
    expansion = keyword_candidates[:keyword_limit] + list(term_signals.keys())
    expansion += INTERDISCIPLINARY_TERMS
    pool: List[str] = []
    seen: Set[str] = set()
    for concept in seed_concepts + expansion:
        if concept not in seen:
            pool.append(concept)
            seen.add(concept)
    return pool


def serialize_thought(thought: Any) -> Dict[str, Any]:
    return {
        "id": thought.id,
        "content": thought.content,
        "thought_type": thought.thought_type.name,
        "snr_score": thought.get_snr_score(),
        "domains": sorted(thought.domains),
        "depth": thought.depth,
        "parent_thoughts": thought.parent_thoughts,
    }


def serialize_bridge(bridge: Any) -> Dict[str, Any]:
    return {
        "id": bridge.id,
        "bridge_type": bridge.bridge_type.name,
        "source_domain": bridge.source_domain,
        "target_domain": bridge.target_domain,
        "source_concept": bridge.source_concept,
        "target_concept": bridge.target_concept,
        "strength": bridge.strength,
        "novelty": bridge.novelty,
        "snr_score": bridge.snr_score,
    }


def serialize_chain(chain: Any) -> Dict[str, Any]:
    return {
        "id": chain.id,
        "total_snr": chain.total_snr,
        "avg_snr": chain.avg_snr,
        "max_depth": chain.max_depth,
        "domain_diversity": chain.domain_diversity,
        "thought_count": len(chain.thoughts),
        "bridge_count": len(chain.bridges),
        "thoughts": [serialize_thought(t) for t in chain.thoughts],
        "bridges": [serialize_bridge(b) for b in chain.bridges],
    }


def serialize_wisdom(wisdom: Any) -> Dict[str, Any]:
    return {
        "id": wisdom.id,
        "wisdom_type": wisdom.wisdom_type.name,
        "title": wisdom.title,
        "snr_score": wisdom.snr_score,
        "confidence": wisdom.confidence,
        "domains": sorted(wisdom.domain_tags),
    }


def serialize_enhanced_chain(enhanced: Any) -> Dict[str, Any]:
    return {
        "enhanced_snr": enhanced.enhanced_snr,
        "giants_boost": enhanced.giants_boost,
        "cross_temporal_bridges": enhanced.cross_temporal_bridges,
        "flywheel_momentum": enhanced.flywheel_momentum,
        "chain": serialize_chain(enhanced.chain),
        "wisdom_augmentations": [serialize_wisdom(w) for w in enhanced.wisdom_augmentations],
    }


async def run_pipeline(
    args: argparse.Namespace,
    memory_system: Optional[AgentMemorySystem] = None,
) -> Tuple[Dict[str, Any], List[Any], Any]:
    """
    Run the Giantsâ†’GoT pipeline and return payload, raw chains, and engine.

    Returns:
        Tuple of (serialized_payload, enhanced_chains, giants_engine)
        for downstream memory injection.
    """
    chat_dir = Path(args.chat_dir)
    profile_path = Path(args.profile_path)
    profile_data = build_profile(chat_dir, profile_path, args.skip_ingest)

    snr_scorer = SNRScorer()
    giants_engine = create_giants_engine()

    trace = build_reasoning_trace(profile_data, args.query)
    await giants_engine.extract_wisdom_from_trace(
        reasoning_trace=trace,
        source=WisdomSource.LOCAL,
    )

    hub_concepts = [h["concept"] for h in giants_engine.get_hub_concepts(top_k=args.hub_topics)]
    top_topics = list(profile_data.get("top_topics", {}).keys())[: args.seed_topics]
    seed_concepts: List[str] = []
    seen: Set[str] = set()
    for concept in top_topics + hub_concepts:
        if concept not in seen:
            seed_concepts.append(concept)
            seen.add(concept)

    hub_domains = {
        concept: data["domains"] for concept, data in giants_engine.HUB_CONCEPTS.items()
    }
    concept_pool = build_concept_pool(
        seed_concepts=seed_concepts,
        profile_data=profile_data,
        term_signals=giants_engine.get_term_signals(),
    )
    domains_by_concept = build_domain_map(concept_pool, hub_domains)
    adjacency = build_adjacency(concept_pool, domains_by_concept)

    async def hypergraph_query_fn(node: str) -> List[Dict[str, Any]]:
        return adjacency.get(node, [])

    async def convergence_fn(concept: str, context: Dict[str, Any]) -> ConvergenceResult:
        depth = int(context.get("depth", 0))
        clarity = max(0.5, 0.9 - depth * 0.08)
        synergy = min(0.95, 0.6 + depth * 0.05)
        entropy = min(0.5, 0.2 + depth * 0.05)
        quantization_error = min(0.3, 0.05 + depth * 0.02)
        mutual_information = min(0.9, 0.6 + depth * 0.05)
        quality = ConvergenceQuality.GOOD if clarity >= 0.75 else ConvergenceQuality.ACCEPTABLE
        return ConvergenceResult(
            clarity=clarity,
            mutual_information=mutual_information,
            entropy=entropy,
            synergy=synergy,
            quantization_error=quantization_error,
            quality=quality,
            action={"type": "synthetic_convergence", "concept": concept},
        )

    engine = GiantsEnhancedGoT(
        snr_scorer=snr_scorer,
        giants_engine=giants_engine,
        beam_width=args.beam_width,
        max_depth=args.max_depth,
    )

    domains = sorted({d for doms in (domains_by_concept[c] for c in seed_concepts) for d in doms})
    enhanced_chains = await engine.reason_with_wisdom(
        query=args.query,
        seed_concepts=seed_concepts,
        hypergraph_query_fn=hypergraph_query_fn,
        convergence_fn=convergence_fn,
        top_k_chains=args.top_k_chains,
        resource_budget=args.resource_budget,
        domains=domains,
    )

    evidence_pack = engine.generate_evidence_pack()
    serialized_chains = [serialize_enhanced_chain(c) for c in enhanced_chains]
    summary = summarize_profile(profile_data)

    payload = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "query": args.query,
        "profile_summary": summary,
        "seed_concepts": seed_concepts,
        "domains": domains,
        "enhanced_chains": serialized_chains,
        "evidence_pack": evidence_pack,
    }

    return payload, enhanced_chains, giants_engine


def write_outputs(output_dir: Path, payload: Dict[str, Any]) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    evidence_pack = payload.get("evidence_pack", {})
    chains = payload.get("enhanced_chains", [])

    evidence_path = output_dir / "EVIDENCE_PACK.json"
    chains_path = output_dir / "ENHANCED_CHAINS.json"
    summary_path = output_dir / "RUN_SUMMARY.md"

    with evidence_path.open("w", encoding="utf-8") as handle:
        json.dump(evidence_pack, handle, indent=2, ensure_ascii=True)
    with chains_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, ensure_ascii=True)

    top_chain = chains[0] if chains else {}
    summary_lines = [
        "# GIANTS-GOT Autonomous Run Summary",
        f"timestamp: {payload.get('timestamp')}",
        f"query: {payload.get('query')}",
        f"messages_analyzed: {payload.get('profile_summary', {}).get('total_messages_analyzed', 0)}",
        f"seed_concepts: {', '.join(payload.get('seed_concepts', []))}",
        f"top_enhanced_snr: {top_chain.get('enhanced_snr', 'n/a')}",
        f"top_chain_thoughts: {top_chain.get('chain', {}).get('thought_count', 0)}",
        f"top_chain_bridges: {top_chain.get('chain', {}).get('bridge_count', 0)}",
        f"evidence_pack: {evidence_path.name}",
        f"chains_output: {chains_path.name}",
    ]
    with summary_path.open("w", encoding="utf-8") as handle:
        handle.write("\n".join(summary_lines))


def main() -> int:
    args = parse_args()
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO))

    timestamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    output_dir = (
        Path(args.output_dir)
        if args.output_dir
        else Path("evidence/packs") / f"PACK-GIANTS-AUTO-{timestamp}"
    )

    # Create SNR scorer (shared between memory and pipeline)
    snr_scorer = SNRScorer()

    # Load existing memory snapshot if persistence is enabled
    memory_system: Optional[AgentMemorySystem] = None
    snapshot_path = Path(args.memory_snapshot)
    if not args.no_memory_persist:
        memory_system = load_memory_snapshot(snapshot_path, snr_scorer)
        if memory_system is None:
            memory_system = AgentMemorySystem(snr_scorer=snr_scorer)
            logger.info("Initialized fresh memory system for pipeline.")

    # Run the pipeline
    payload, enhanced_chains, giants_engine = asyncio.run(
        run_pipeline(args, memory_system)
    )
    write_outputs(output_dir, payload)

    # Inject extracted wisdom into memory and persist
    if memory_system is not None and not args.no_memory_persist:
        injected = asyncio.run(
            inject_wisdom_to_memory(memory_system, enhanced_chains, giants_engine)
        )
        logger.info("Injected %d items into memory system.", injected)
        save_memory_snapshot(memory_system, snapshot_path)

    logger.info("Artifacts written to %s", output_dir)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
