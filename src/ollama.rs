// src/ollama.rs - Ollama Local LLM Integration
//
// Connects to local Ollama instance for:
// - Text generation (generate endpoint)
// - Chat completion (chat endpoint)
// - Embeddings generation (embeddings endpoint)
//
// Environment variables:
// - OLLAMA_URL: Base URL (default: http://localhost:11434)
// - OLLAMA_MODEL: Default model (default: llama3.2)

use lazy_static::lazy_static;
use prometheus::{
    register_counter_vec, register_gauge, register_histogram_vec, CounterVec, Gauge, HistogramVec,
};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::OnceCell;
use tracing::{debug, error, info, instrument, warn};

/// Default Ollama server URL
const DEFAULT_OLLAMA_URL: &str = "http://localhost:11434";

/// Default model for generation
const DEFAULT_MODEL: &str = "llama3.2";

/// Request timeout for Ollama API calls
const REQUEST_TIMEOUT: Duration = Duration::from_secs(120);

/// Maximum tokens for generation (reserved for future use)
#[allow(dead_code)]
const DEFAULT_MAX_TOKENS: u32 = 2048;

lazy_static! {
    /// Ollama API call counter
    pub static ref OLLAMA_REQUESTS: CounterVec = register_counter_vec!(
        "bizra_ollama_requests_total",
        "Total Ollama API requests",
        &["endpoint", "model", "result"]  // generate/chat/embeddings, model name, success/error
    ).unwrap();

    /// Ollama generation latency
    pub static ref OLLAMA_LATENCY: HistogramVec = register_histogram_vec!(
        "bizra_ollama_latency_seconds",
        "Ollama API latency in seconds",
        &["endpoint", "model"],
        vec![0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0]
    ).unwrap();

    /// Ollama connection status
    pub static ref OLLAMA_CONNECTED: Gauge = register_gauge!(
        "bizra_ollama_connected",
        "Ollama connection status (1=connected, 0=disconnected)"
    ).unwrap();

    /// Ollama tokens generated
    pub static ref OLLAMA_TOKENS: CounterVec = register_counter_vec!(
        "bizra_ollama_tokens_total",
        "Total tokens generated by Ollama",
        &["type", "model"]  // prompt/completion, model
    ).unwrap();
}

/// Global Ollama client singleton
static OLLAMA_CLIENT: OnceCell<Arc<OllamaClient>> = OnceCell::const_new();

/// Get or create the global Ollama client
pub async fn get_ollama() -> Arc<OllamaClient> {
    OLLAMA_CLIENT
        .get_or_init(|| async {
            let client = OllamaClient::from_env().await;
            Arc::new(client)
        })
        .await
        .clone()
}

/// Ollama API Error
#[derive(Debug, thiserror::Error)]
pub enum OllamaError {
    #[error("HTTP error: {0}")]
    Http(#[from] reqwest::Error),

    #[error("Invalid response: {0}")]
    InvalidResponse(String),

    #[error("Model not found: {0}")]
    ModelNotFound(String),

    #[error("Connection failed: {0}")]
    ConnectionFailed(String),

    #[error("Generation timeout")]
    Timeout,
}

/// Chat message for Ollama chat API
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String, // system, user, assistant
    pub content: String,
}

impl ChatMessage {
    pub fn system(content: impl Into<String>) -> Self {
        Self {
            role: "system".to_string(),
            content: content.into(),
        }
    }

    pub fn user(content: impl Into<String>) -> Self {
        Self {
            role: "user".to_string(),
            content: content.into(),
        }
    }

    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.into(),
        }
    }
}

/// Generation options
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct GenerationOptions {
    pub temperature: Option<f64>,
    pub top_p: Option<f64>,
    pub top_k: Option<u32>,
    pub num_ctx: Option<u32>,
    pub num_predict: Option<u32>,
    pub repeat_penalty: Option<f64>,
    pub seed: Option<i64>,
    pub stop: Option<Vec<String>>,
}

/// Generate response from Ollama
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenerateResponse {
    pub model: String,
    pub response: String,
    pub done: bool,
    pub context: Option<Vec<i64>>,
    pub total_duration: Option<u64>,
    pub load_duration: Option<u64>,
    pub prompt_eval_count: Option<u32>,
    pub prompt_eval_duration: Option<u64>,
    pub eval_count: Option<u32>,
    pub eval_duration: Option<u64>,
}

/// Chat response from Ollama
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatResponse {
    pub model: String,
    pub message: ChatMessage,
    pub done: bool,
    pub total_duration: Option<u64>,
    pub load_duration: Option<u64>,
    pub prompt_eval_count: Option<u32>,
    pub eval_count: Option<u32>,
}

/// Embeddings response from Ollama
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingsResponse {
    pub model: String,
    pub embeddings: Vec<Vec<f64>>,
    pub total_duration: Option<u64>,
    pub load_duration: Option<u64>,
    pub prompt_eval_count: Option<u32>,
}

/// Model info from Ollama
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub name: String,
    pub model: String,
    pub modified_at: String,
    pub size: u64,
    pub digest: String,
    pub details: Option<ModelDetails>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelDetails {
    pub parent_model: Option<String>,
    pub format: Option<String>,
    pub family: Option<String>,
    pub families: Option<Vec<String>>,
    pub parameter_size: Option<String>,
    pub quantization_level: Option<String>,
}

/// Ollama Client (Hybrid: Supports Native Ollama and OpenAI-Compatible APIs)
#[derive(Clone)]
pub struct OllamaClient {
    base_url: String,
    default_model: String,
    api_key: String,
    api_type: ApiType,
    http: Client,
    connected: bool,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ApiType {
    Ollama,
    OpenAI,
}

impl OllamaClient {
    /// Create new client with explicit configuration
    pub fn new(
        base_url: String,
        default_model: String,
        api_type: ApiType,
        api_key: Option<String>,
    ) -> Self {
        let http = Client::builder()
            .timeout(REQUEST_TIMEOUT)
            .build()
            .expect("Failed to create HTTP client");

        Self {
            base_url,
            default_model,
            api_key: api_key.unwrap_or_default(),
            api_type,
            http,
            connected: false,
        }
    }

    /// Create client from environment variables
    pub async fn from_env() -> Self {
        let base_url =
            std::env::var("OLLAMA_URL").unwrap_or_else(|_| DEFAULT_OLLAMA_URL.to_string());
        let default_model =
            std::env::var("OLLAMA_MODEL").unwrap_or_else(|_| DEFAULT_MODEL.to_string());
        let api_type_str =
            std::env::var("OLLAMA_API_TYPE").unwrap_or_else(|_| "ollama".to_string());
        let api_key = std::env::var("OLLAMA_API_KEY").ok();

        let api_type = match api_type_str.to_lowercase().as_str() {
            "openai" | "lmstudio" | "vllm" => ApiType::OpenAI,
            _ => ApiType::Ollama,
        };

        let mut client = Self::new(base_url.clone(), default_model, api_type, api_key);

        // Test connection
        match client.health_check().await {
            Ok(_) => {
                info!(
                    "ðŸ¦™ Local LLM connected at {} ({:?})",
                    base_url, client.api_type
                );
                client.connected = true;
                OLLAMA_CONNECTED.set(1.0);
            }
            Err(e) => {
                warn!("âš ï¸ Local LLM not available at {}: {}", base_url, e);
                client.connected = false;
                OLLAMA_CONNECTED.set(0.0);
            }
        }

        client
    }

    /// Check if Ollama is connected
    pub fn is_connected(&self) -> bool {
        self.connected
    }

    /// Health check - verify Ollama is running
    #[instrument(skip(self))]
    pub async fn health_check(&self) -> Result<(), OllamaError> {
        let url = match self.api_type {
            ApiType::Ollama => format!("{}/api/tags", self.base_url),
            ApiType::OpenAI => format!("{}/v1/models", self.base_url.trim_end_matches("/v1")),
        };

        let mut req = self.http.get(&url);
        if !self.api_key.is_empty() {
            req = req.header("Authorization", format!("Bearer {}", self.api_key));
        }

        let resp = req.send().await?;

        if resp.status().is_success() {
            Ok(())
        } else {
            Err(OllamaError::ConnectionFailed(format!(
                "Status: {}",
                resp.status()
            )))
        }
    }

    /// List available models
    #[instrument(skip(self))]
    pub async fn list_models(&self) -> Result<Vec<ModelInfo>, OllamaError> {
        let url = match self.api_type {
            ApiType::Ollama => format!("{}/api/tags", self.base_url),
            ApiType::OpenAI => format!("{}/v1/models", self.base_url.trim_end_matches("/v1")),
        };

        let mut req = self.http.get(&url);
        if !self.api_key.is_empty() {
            req = req.header("Authorization", format!("Bearer {}", self.api_key));
        }

        let resp = req.send().await?;
        let data: serde_json::Value = resp.json().await?;

        // Logic to parse OpenAI models vs Ollama models...
        // For now, if OpenAI, just return empty or minimal list since we rely on configured model
        if matches!(self.api_type, ApiType::OpenAI) {
            // Basic parsing of OpenAI /models response
            let models: Vec<ModelInfo> = data
                .get("data")
                .and_then(|v| v.as_array())
                .map(|arr| {
                    arr.iter()
                        .map(|m| ModelInfo {
                            name: m["id"].as_str().unwrap_or("unknown").to_string(),
                            model: m["id"].as_str().unwrap_or("unknown").to_string(),
                            modified_at: "unknown".to_string(),
                            size: 0,
                            digest: "unknown".to_string(),
                            details: None,
                        })
                        .collect()
                })
                .unwrap_or_default();
            return Ok(models);
        }

        let models: Vec<ModelInfo> = data
            .get("models")
            .and_then(|m| serde_json::from_value(m.clone()).ok())
            .unwrap_or_default();

        debug!("Found {} Ollama models", models.len());
        Ok(models)
    }

    /// Generate text completion
    #[instrument(skip(self, options))]
    pub async fn generate(
        &self,
        prompt: &str,
        model: Option<&str>,
        options: Option<GenerationOptions>,
    ) -> Result<GenerateResponse, OllamaError> {
        let start = Instant::now();
        let model = model.unwrap_or(&self.default_model);

        match self.api_type {
            ApiType::OpenAI => self.generate_openai(prompt, model, options, start).await,
            ApiType::Ollama => self.generate_ollama(prompt, model, options, start).await,
        }
    }

    async fn generate_ollama(
        &self,
        prompt: &str,
        model: &str,
        options: Option<GenerationOptions>,
        start: Instant,
    ) -> Result<GenerateResponse, OllamaError> {
        let url = format!("{}/api/generate", self.base_url);

        let mut body = serde_json::json!({
            "model": model,
            "prompt": prompt,
            "stream": false,
        });

        // Apply options
        if let Some(opts) = options {
            let mut options_map = serde_json::Map::new();
            if let Some(t) = opts.temperature {
                options_map.insert("temperature".to_string(), serde_json::json!(t));
            }
            if let Some(p) = opts.top_p {
                options_map.insert("top_p".to_string(), serde_json::json!(p));
            }
            if let Some(k) = opts.top_k {
                options_map.insert("top_k".to_string(), serde_json::json!(k));
            }
            if let Some(ctx) = opts.num_ctx {
                options_map.insert("num_ctx".to_string(), serde_json::json!(ctx));
            }
            if let Some(predict) = opts.num_predict {
                options_map.insert("num_predict".to_string(), serde_json::json!(predict));
            }
            if let Some(penalty) = opts.repeat_penalty {
                options_map.insert("repeat_penalty".to_string(), serde_json::json!(penalty));
            }
            if let Some(seed) = opts.seed {
                options_map.insert("seed".to_string(), serde_json::json!(seed));
            }
            if !options_map.is_empty() {
                body["options"] = serde_json::Value::Object(options_map);
            }
            if let Some(stop) = opts.stop {
                body["stop"] = serde_json::json!(stop);
            }
        }

        debug!("Calling Ollama generate: model={}", model);

        let resp = self.http.post(&url).json(&body).send().await?;
        let latency = start.elapsed();

        OLLAMA_LATENCY
            .with_label_values(&["generate", model])
            .observe(latency.as_secs_f64());

        if !resp.status().is_success() {
            let status = resp.status();
            let error_text = resp.text().await.unwrap_or_default();
            OLLAMA_REQUESTS
                .with_label_values(&["generate", model, "error"])
                .inc();

            if error_text.contains("model") && error_text.contains("not found") {
                return Err(OllamaError::ModelNotFound(model.to_string()));
            }

            return Err(OllamaError::InvalidResponse(format!(
                "Status {}: {}",
                status, error_text
            )));
        }

        let response: GenerateResponse = resp.json().await?;

        // Record metrics
        OLLAMA_REQUESTS
            .with_label_values(&["generate", model, "success"])
            .inc();

        if let Some(prompt_tokens) = response.prompt_eval_count {
            OLLAMA_TOKENS
                .with_label_values(&["prompt", model])
                .inc_by(prompt_tokens as f64);
        }
        if let Some(completion_tokens) = response.eval_count {
            OLLAMA_TOKENS
                .with_label_values(&["completion", model])
                .inc_by(completion_tokens as f64);
        }

        debug!(
            "Ollama generate completed in {:.2}s, {} tokens",
            latency.as_secs_f64(),
            response.eval_count.unwrap_or(0)
        );

        Ok(response)
    }

    /// Chat completion with message history
    #[instrument(skip(self, messages, options))]
    pub async fn chat(
        &self,
        messages: Vec<ChatMessage>,
        model: Option<&str>,
        options: Option<GenerationOptions>,
    ) -> Result<ChatResponse, OllamaError> {
        let start = Instant::now();
        let model = model.unwrap_or(&self.default_model);

        match self.api_type {
            ApiType::OpenAI => self.chat_openai(messages, model, options, start).await,
            ApiType::Ollama => self.chat_ollama(messages, model, options, start).await,
        }
    }

    async fn chat_ollama(
        &self,
        messages: Vec<ChatMessage>,
        model: &str,
        options: Option<GenerationOptions>,
        start: Instant,
    ) -> Result<ChatResponse, OllamaError> {
        let url = format!("{}/api/chat", self.base_url);

        let mut body = serde_json::json!({
            "model": model,
            "messages": messages,
            "stream": false,
        });

        // Apply options
        if let Some(opts) = options {
            let mut options_map = serde_json::Map::new();
            if let Some(t) = opts.temperature {
                options_map.insert("temperature".to_string(), serde_json::json!(t));
            }
            if let Some(p) = opts.top_p {
                options_map.insert("top_p".to_string(), serde_json::json!(p));
            }
            if let Some(k) = opts.top_k {
                options_map.insert("top_k".to_string(), serde_json::json!(k));
            }
            if let Some(ctx) = opts.num_ctx {
                options_map.insert("num_ctx".to_string(), serde_json::json!(ctx));
            }
            if let Some(predict) = opts.num_predict {
                options_map.insert("num_predict".to_string(), serde_json::json!(predict));
            }
            if let Some(penalty) = opts.repeat_penalty {
                options_map.insert("repeat_penalty".to_string(), serde_json::json!(penalty));
            }
            if let Some(seed) = opts.seed {
                options_map.insert("seed".to_string(), serde_json::json!(seed));
            }
            if !options_map.is_empty() {
                body["options"] = serde_json::Value::Object(options_map);
            }
            if let Some(stop) = opts.stop {
                body["stop"] = serde_json::json!(stop);
            }
        }

        let resp = self.http.post(&url).json(&body).send().await?;
        let latency = start.elapsed();

        OLLAMA_LATENCY
            .with_label_values(&["chat", model])
            .observe(latency.as_secs_f64());

        if !resp.status().is_success() {
            let status = resp.status();
            let error_text = resp.text().await.unwrap_or_default();
            OLLAMA_REQUESTS
                .with_label_values(&["chat", model, "error"])
                .inc();

            if error_text.contains("model") && error_text.contains("not found") {
                return Err(OllamaError::ModelNotFound(model.to_string()));
            }

            return Err(OllamaError::InvalidResponse(format!(
                "Status {}: {}",
                status, error_text
            )));
        }

        let response: ChatResponse = resp.json().await?;

        OLLAMA_REQUESTS
            .with_label_values(&["chat", model, "success"])
            .inc();

        if let Some(prompt_tokens) = response.prompt_eval_count {
            OLLAMA_TOKENS
                .with_label_values(&["prompt", model])
                .inc_by(prompt_tokens as f64);
        }
        if let Some(completion_tokens) = response.eval_count {
            OLLAMA_TOKENS
                .with_label_values(&["completion", model])
                .inc_by(completion_tokens as f64);
        }

        debug!(
            "Ollama chat completed in {:.2}s, {} completion tokens",
            latency.as_secs_f64(),
            response.eval_count.unwrap_or(0)
        );

        Ok(response)
    }

    async fn chat_openai(
        &self,
        messages: Vec<ChatMessage>,
        model: &str,
        options: Option<GenerationOptions>,
        start: Instant,
    ) -> Result<ChatResponse, OllamaError> {
        let url = format!(
            "{}/v1/chat/completions",
            self.base_url.trim_end_matches("/v1")
        );

        let mut body = serde_json::json!({
            "model": model,
            "messages": messages,
            "stream": false,
        });

        if let Some(opts) = options {
            if let Some(t) = opts.temperature {
                body["temperature"] = serde_json::json!(t);
            }
        }

        let mut req = self.http.post(&url);
        if !self.api_key.is_empty() {
            req = req.header("Authorization", format!("Bearer {}", self.api_key));
        }

        let resp = req.json(&body).send().await?;
        let data: serde_json::Value = resp.json().await?;

        let content = data["choices"][0]["message"]["content"]
            .as_str()
            .unwrap_or_default();
        let role = data["choices"][0]["message"]["role"]
            .as_str()
            .unwrap_or("assistant");

        let prompt_tokens = data["usage"]["prompt_tokens"].as_u64().map(|u| u as u32);
        let completion_tokens = data["usage"]["completion_tokens"]
            .as_u64()
            .map(|u| u as u32);

        Ok(ChatResponse {
            model: model.to_string(),
            message: ChatMessage {
                role: role.to_string(),
                content: content.to_string(),
            },
            done: true,
            total_duration: Some(start.elapsed().as_nanos() as u64),
            load_duration: None,
            prompt_eval_count: prompt_tokens,
            eval_count: completion_tokens,
        })
    }

    // START OF APPENDED generate_openai (moved inside impl)
    async fn generate_openai(
        &self,
        prompt: &str,
        model: &str,
        options: Option<GenerationOptions>,
        start: Instant,
    ) -> Result<GenerateResponse, OllamaError> {
        let url = format!("{}/v1/completions", self.base_url.trim_end_matches("/v1"));

        // Map options to OpenAI format
        let mut body = serde_json::json!({
            "model": model,
            "prompt": prompt,
            "stream": false,
        });

        if let Some(opts) = options {
            if let Some(t) = opts.temperature {
                body["temperature"] = serde_json::json!(t);
            }
            if let Some(p) = opts.top_p {
                body["top_p"] = serde_json::json!(p);
            }
            if let Some(max) = opts.num_predict {
                body["max_tokens"] = serde_json::json!(max);
            }
            if let Some(stop) = opts.stop {
                body["stop"] = serde_json::json!(stop);
            }
        }

        let mut req = self.http.post(&url);
        if !self.api_key.is_empty() {
            req = req.header("Authorization", format!("Bearer {}", self.api_key));
        }

        let resp = req.json(&body).send().await?;

        if !resp.status().is_success() {
            return Err(OllamaError::InvalidResponse(format!(
                "OpenAI Status: {}",
                resp.status()
            )));
        }

        let data: serde_json::Value = resp.json().await?;

        let text = data["choices"][0]["text"]
            .as_str()
            .unwrap_or_default()
            .to_string();
        let prompt_tokens = data["usage"]["prompt_tokens"].as_u64().map(|u| u as u32);
        let completion_tokens = data["usage"]["completion_tokens"]
            .as_u64()
            .map(|u| u as u32);

        Ok(GenerateResponse {
            model: model.to_string(),
            response: text,
            done: true,
            context: None,
            total_duration: Some(start.elapsed().as_nanos() as u64),
            load_duration: None,
            prompt_eval_count: prompt_tokens,
            prompt_eval_duration: None,
            eval_count: completion_tokens,
            eval_duration: None,
        })
    }
}
impl OllamaClient {
    // Legacy helper wrappers for HTTP layer
    pub async fn bizra_generate(
        &self,
        prompt: &str,
        model: Option<&str>,
    ) -> Result<GenerateResponse, OllamaError> {
        self.generate(prompt, model, None).await
    }

    pub async fn bizra_chat(
        &self,
        message: &str,
        history: Vec<ChatMessage>, // Ignored in simple wrapper if logic changed, else used
        model: Option<&str>,
    ) -> Result<ChatResponse, OllamaError> {
        // Simple append for now
        let mut messages = history;
        messages.push(ChatMessage::user(message));
        self.chat(messages, model, None).await
    }
}
